{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:   #F91818 ;\"> <b> Лабораторная работа №7 </h1>\n",
    "<h3 style=\"color:   #FB6868 ;\"> <b> Работа с текстовыми данными </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:   #F6ACAC ;\"> 1. Пример, описанный в ходе работы </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "reviews_train = load_files(\"aclImdb/train/\")\n",
    "# load_files возвращает коллекцию, содержащую обучающие тексты и обучающие метки\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "\n",
    "reviews_test = load_files(\"aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\bigdatata\\laba7\\Laba7_KalmanovNurdin.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/bigdatata/laba7/Laba7_KalmanovNurdin.ipynb#X65sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lda \u001b[39m=\u001b[39m LatentDirichletAllocation(n_components\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, learning_method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/bigdatata/laba7/Laba7_KalmanovNurdin.ipynb#X65sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Мы строим модель и преобразуем данные в один этап\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/bigdatata/laba7/Laba7_KalmanovNurdin.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Преобразование займет некоторое время,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/bigdatata/laba7/Laba7_KalmanovNurdin.ipynb#X65sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# и мы можем сэкономить время, выполнив обе операции сразу\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/bigdatata/laba7/Laba7_KalmanovNurdin.ipynb#X65sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m document_topics \u001b[39m=\u001b[39m lda\u001b[39m.\u001b[39;49mfit_transform(X)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:673\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_em_step(\n\u001b[0;32m    666\u001b[0m             X[idx_slice, :],\n\u001b[0;32m    667\u001b[0m             total_samples\u001b[39m=\u001b[39mn_samples,\n\u001b[0;32m    668\u001b[0m             batch_update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    669\u001b[0m             parallel\u001b[39m=\u001b[39mparallel,\n\u001b[0;32m    670\u001b[0m         )\n\u001b[0;32m    671\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    672\u001b[0m     \u001b[39m# batch update\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_em_step(\n\u001b[0;32m    674\u001b[0m         X, total_samples\u001b[39m=\u001b[39;49mn_samples, batch_update\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, parallel\u001b[39m=\u001b[39;49mparallel\n\u001b[0;32m    675\u001b[0m     )\n\u001b[0;32m    677\u001b[0m \u001b[39m# check perplexity\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m evaluate_every \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m evaluate_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:524\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[1;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \n\u001b[0;32m    499\u001b[0m \u001b[39mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[39m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m# E-step\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m _, suff_stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_e_step(\n\u001b[0;32m    525\u001b[0m     X, cal_sstats\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, random_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, parallel\u001b[39m=\u001b[39;49mparallel\n\u001b[0;32m    526\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39m# M-step\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39mif\u001b[39;00m batch_update:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:467\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[1;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m parallel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m--> 467\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    468\u001b[0m     delayed(_update_doc_distribution)(\n\u001b[0;32m    469\u001b[0m         X[idx_slice, :],\n\u001b[0;32m    470\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexp_dirichlet_component_,\n\u001b[0;32m    471\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc_topic_prior_,\n\u001b[0;32m    472\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_doc_update_iter,\n\u001b[0;32m    473\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean_change_tol,\n\u001b[0;32m    474\u001b[0m         cal_sstats,\n\u001b[0;32m    475\u001b[0m         random_state,\n\u001b[0;32m    476\u001b[0m     )\n\u001b[0;32m    477\u001b[0m     \u001b[39mfor\u001b[39;49;00m idx_slice \u001b[39min\u001b[39;49;00m gen_even_slices(X\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], n_jobs)\n\u001b[0;32m    478\u001b[0m )\n\u001b[0;32m    480\u001b[0m \u001b[39m# merge result\u001b[39;00m\n\u001b[0;32m    481\u001b[0m doc_topics, sstats_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:144\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[1;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[0;32m    140\u001b[0m last_d \u001b[39m=\u001b[39m doc_topic_d\n\u001b[0;32m    142\u001b[0m \u001b[39m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m norm_phi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(exp_doc_topic_d, exp_topic_word_d) \u001b[39m+\u001b[39m eps\n\u001b[0;32m    146\u001b[0m doc_topic_d \u001b[39m=\u001b[39m exp_doc_topic_d \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mdot(cnts \u001b[39m/\u001b[39m norm_phi, exp_topic_word_d\u001b[39m.\u001b[39mT)\n\u001b[0;32m    147\u001b[0m \u001b[39m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\", max_iter=10, random_state=0)\n",
    "\n",
    "# Мы строим модель и преобразуем данные в один этап\n",
    "# Преобразование займет некоторое время,\n",
    "# и мы можем сэкономить время, выполнив обе операции сразу\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Для каждой темы (строки в components_) сортируем признаки (по возрастанию) # Инвертируем строки с помощью [:, ::-1], чтобы получить сортировку по убыванию\n",
    "sorting =np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# Получаем имена признаков из векторизатора\n",
    "feature_names = np.array(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "# Выводим 10 тем:\n",
    "mglearn.tools.print_topics (topics=range (10), feature_names=feature_names, \n",
    "                            sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda100 = LatentDirichletAllocation (n_components=100, learning_method=\"batch\", max_iter=10, random_state=0)\n",
    "document_topics100 = lda100.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "topics = np.array([7,16,24,25,28,36,37,45,51,53,54,63,89,97])\n",
    "\n",
    "sorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "mglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n",
    "                           sorting=sorting,topics_per_chunk=7,n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сортируем документы по весу темы 45 \"музыка\"\n",
    "music = np.argsort(document_topics100[:, 45])[::-1]\n",
    "\n",
    "# печатаем 10 документов, в которых данная тема является наиболее важной\n",
    "for i in music[:10]: # выводим первые два предложения\n",
    "    print (b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "topic_names = [\"{:>2} \".format(i) + \" \".join(words) \n",
    "               for i, words in enumerate(feature_names[sorting[:, :2]])]\n",
    "# two column bar chart:\n",
    "\n",
    "for col in [0, 1]:\n",
    "\n",
    "    start = col * 50\n",
    "    end = (col + 1) * 50\n",
    "    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end]) \n",
    "    ax[col].set_yticks(np.arange(50))\n",
    "    ax[col].set_yticklabels (topic_names[start:end], ha=\"left\", va=\"top\")\n",
    "    ax[col].invert_yaxis()\n",
    "    ax[col].set_xlim(0, 2000)\n",
    "    yax = ax[col].get_yaxis()\n",
    "    yax.set_tick_params(pad=130)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:   #F6ACAC ;\"> 2. Создание «мешка слов», например, набор из 20-25 отзывов о чем-либо и краткое пояснение к данным. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание своего «мешка слов»\n",
    "my_reviews = [\n",
    "    \"Отличный фильм! Очень понравился сюжет и актерская игра.\",\n",
    "    \"Не понравился финал, ожидал чего-то более неожиданного.\",\n",
    "    \"Сюжет слабый, но визуальная составляющая поражает.\",\n",
    "    \"Замечательный фильм для всей семьи, смешно и трогательно.\",\n",
    "    \"Фильм оставил двоякое впечатление, некоторые моменты поразили, но есть и неясности.\",\n",
    "    \"Отличная режиссерская работа, актеры справились на отлично!\",\n",
    "    \"Не смог оторваться от экрана, интригующий сюжет до последней минуты.\",\n",
    "    \"Фильм подарил мне кучу положительных эмоций, спасибо за такое кино!\",\n",
    "    \"Сюжет затянут, несмотря на это, фильм оставил приятное впечатление.\",\n",
    "    \"Эффектные визуальные эффекты, но где-то теряется смысл сюжета.\",\n",
    "    \"Ожидал большего, но не получил. Впечатление разочарования.\",\n",
    "    \"Фильм для тех, кто ценит глубокий смысл и хорошую игру актеров.\",\n",
    "    \"Замечательное сочетание драмы и юмора, смотрится на одном дыхании.\",\n",
    "    \"Необычный подход к теме, но не у всех он вызовет восторг.\",\"Фильм, который оставил меня в раздумьях, интересные философские аспекты.\",\n",
    "    \"Сценарий поражает своей оригинальностью, необычные повороты сюжета.\",\n",
    "    \"Атмосфера фильма так и пронизана таинственностью, что создает неповторимый опыт.\",\n",
    "    \"Не для каждого, но любителям тяжелых драм рекомендую.\",\n",
    "    \"Зрелищные боевые сцены и отличная работа художников по костюмам.\",\n",
    "    \"Неплохой психологический триллер, но финал оставил вопросы.\",\n",
    "    \"Фильм смог перенести меня в другой мир, волшебство на экране.\",\n",
    "    \"Позитивный настрой и отличный юмор, идеальный выбор для веселого вечера.\",\n",
    "    \"Игра актеров на высшем уровне, каждый персонаж - настоящая жемчужина.\",\n",
    "    \"Оригинальный подход к жанру, стоит посмотреть для разнообразия.\",\"Фильм, который заставил задуматься о смысле жизни, глубокий и трогательный.\",\n",
    "    \"Отличная игра актеров, особенно впечатлил перформанс главного героя.\",\n",
    "    \"Сюжет развивается неспешно, но это лишь добавляет загадочности и интриги.\",\n",
    "    \"Необычный жанровый микс, создает интересное визуальное восприятие.\",\n",
    "    \"Картина о любви, которая оставила слезы на глазах.\",\n",
    "    \"Блестящая характерная музыка, идеально сочетающаяся с общей атмосферой.\",\n",
    "    \"Фильм, который вызывает смех и грусть, настоящая эмоциональная ачивка.\",\n",
    "    \"Необычная концовка, которая оставляет много места для интерпретации.\",\n",
    "    \"Браво режиссеру за отличную работу с атмосферой и цветовой палитрой.\",\n",
    "    \"Фильм, который нельзя забыть, завораживает своей непредсказуемостью.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датафрейма\n",
    "data = pd.DataFrame({\"Review\": my_reviews})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:   #F6ACAC ;\">3. Создание словаря стоп-слов, применение стоп-слова к «мешку слов»</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Загрузка ресурса vader_lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Применение стоп-слов к «мешку слов»\n",
    "data['Review'] = data['Review'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:   #F6ACAC ;\">4. Масштабирование данных с помощью tf-idf</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\bigdatata\\laba7\\Laba7_KalmanovNurdin.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/bigdatata/laba7/Laba7_KalmanovNurdin.ipynb#Y116sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Обучение модели линейной регрессии\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/bigdatata/laba7/Laba7_KalmanovNurdin.ipynb#Y116sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m LinearRegression()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/bigdatata/laba7/Laba7_KalmanovNurdin.ipynb#Y116sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, X_train)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:693\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[39m# Note that neither _rescale_data nor the rest of the fit method of\u001b[39;00m\n\u001b[0;32m    689\u001b[0m \u001b[39m# LinearRegression can benefit from in-place operations when X is a\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[39m# sparse matrix. Therefore, let's not copy X when it is sparse.\u001b[39;00m\n\u001b[0;32m    691\u001b[0m copy_X_in_preprocess_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39missparse(X)\n\u001b[1;32m--> 693\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[0;32m    694\u001b[0m     X,\n\u001b[0;32m    695\u001b[0m     y,\n\u001b[0;32m    696\u001b[0m     fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m    697\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy_X_in_preprocess_data,\n\u001b[0;32m    698\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    699\u001b[0m )\n\u001b[0;32m    701\u001b[0m \u001b[39mif\u001b[39;00m has_sw:\n\u001b[0;32m    702\u001b[0m     \u001b[39m# Sample weight can be implemented via a simple rescaling. Note\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     \u001b[39m# that we safely do inplace rescaling when _preprocess_data has\u001b[39;00m\n\u001b[0;32m    704\u001b[0m     \u001b[39m# already made a copy if requested.\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     X, y, sample_weight_sqrt \u001b[39m=\u001b[39m _rescale_data(\n\u001b[0;32m    706\u001b[0m         X, y, sample_weight, inplace\u001b[39m=\u001b[39mcopy_X_in_preprocess_data\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:239\u001b[0m, in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, copy_y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[0;32m    238\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, copy\u001b[39m=\u001b[39mcopy, accept_sparse\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m], dtype\u001b[39m=\u001b[39mFLOAT_DTYPES)\n\u001b[1;32m--> 239\u001b[0m     y \u001b[39m=\u001b[39m check_array(y, dtype\u001b[39m=\u001b[39;49mX\u001b[39m.\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy_y, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mastype(X\u001b[39m.\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy_y)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:881\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39missparse(array):\n\u001b[0;32m    880\u001b[0m     _ensure_no_complex_data(array)\n\u001b[1;32m--> 881\u001b[0m     array \u001b[39m=\u001b[39m _ensure_sparse_format(\n\u001b[0;32m    882\u001b[0m         array,\n\u001b[0;32m    883\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    884\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    885\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    886\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    887\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    888\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    889\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    890\u001b[0m     )\n\u001b[0;32m    891\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    892\u001b[0m     \u001b[39m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     \u001b[39m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[39m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[0;32m    895\u001b[0m     \u001b[39m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[0;32m    896\u001b[0m     \u001b[39m# of warnings context manager.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:532\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    529\u001b[0m _check_large_sparse(spmatrix, accept_large_sparse)\n\u001b[0;32m    531\u001b[0m \u001b[39mif\u001b[39;00m accept_sparse \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    533\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA sparse matrix was passed, but dense \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdata is required. Use X.toarray() to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconvert to a dense numpy array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m     )\n\u001b[0;32m    537\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(accept_sparse, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(accept_sparse) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "# Масштабирование данных с помощью TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Review'])\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test = train_test_split(tfidf_matrix, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели линейной регрессии\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
